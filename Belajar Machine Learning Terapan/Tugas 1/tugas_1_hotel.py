# -*- coding: utf-8 -*-
"""Tugas 1_Hotel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FzYtGaLUQS23WBxQ5FmxqzZzhTU1Kae-

#Import Data
"""

#Impor Pustaka
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

# Pra-pemrosesan & Pemodelan
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import LabelEncoder, RobustScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report

# Model Machine Learning
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

# Model Deep Learning
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Mengatur Tampilan & Peringatan
warnings.filterwarnings('ignore')
sns.set_style('whitegrid')

# Menggunakan kagglehub.dataset_load() yang lebih baru
try:
    data = pd.read_csv('/kaggle/input/hotel-reservations-classification-dataset/Hotel Reservations.csv')
    print("Data loaded from local path.")
except FileNotFoundError:
    print("Local file not found, loading from Kaggle Hub...")
    import kagglehub
    from kagglehub import KaggleDatasetAdapter
    file_path = ""
    data = kagglehub.load_dataset(KaggleDatasetAdapter.PANDAS,
                                  "ahsan81/hotel-reservations-classification-dataset",'Hotel Reservations.csv')

print("Dimensi data:", data.shape)
print("\nContoh data:")
print(data.head())

"""#Data Exploration"""

#Analisis Data Eksploratif (EDA) - Pemeriksaan Awal
print("\nInfo Data Awal:")
data.info()

print("\nPemeriksaan Nilai Kosong:")
print(data.isnull().sum())

# Mengganti nilai avg_price_per_room yang 0 dengan NaN lalu diisi median
data['avg_price_per_room'] = data['avg_price_per_room'].replace(0, np.nan)
median_price = data['avg_price_per_room'].median()
data['avg_price_per_room'].fillna(median_price, inplace=True)
print(f"\nNilai 0 pada 'avg_price_per_room' telah diganti dengan median: {median_price}")

# EDA - Visualisasi
plt.figure(figsize=(15, 10))
plt.subplot(2, 2, 1)
sns.histplot(data['lead_time'], kde=True)
plt.title('Distribusi Lead Time')

plt.subplot(2, 2, 2)
sns.histplot(data['avg_price_per_room'], kde=True)
plt.title('Distribusi Harga Rata-rata per Kamar')

plt.subplot(2, 2, 3)
sns.countplot(x='market_segment_type', hue='booking_status', data=data)
plt.title('Status Booking berdasarkan Tipe Segmen Pasar')
plt.xticks(rotation=45)

plt.subplot(2, 2, 4)
sns.countplot(x='room_type_reserved', hue='booking_status', data=data)
plt.title('Status Booking berdasarkan Tipe Kamar')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

"""## Eksplorasi dan Persiapan Data (EDA & Preprocessing)

* **Kualitas Data** : Dataset yang digunakan berkualitas tinggi, terbukti dengan tidak adanya nilai kosong (null) pada seluruh kolom. Ini menyederhanakan tahap persiapan data secara signifikan.

* **Penanganan Outlier/Nilai Janggal**: Langkah cerdas dilakukan pada fitur `avg_price_per_room`. Nilai 0 pada kolom ini kemungkinan besar adalah anomali atau kesalahan input data (karena kamar gratis sangat tidak umum). Menggantinya dengan nilai median adalah pendekatan yang kuat dan lebih baik daripada menghapus baris data atau menggantinya dengan rata-rata, terutama jika ada outlier.

* **Wawasan dari Visualisasi**:

  * **Distribusi Fitur** : Visualisasi menunjukkan bahwa fitur seperti `lead_time` (jarak waktu antara pemesanan dan kedatangan) dan `avg_price_per_room` memiliki distribusi yang skewed (condong ke kanan), yang mengindikasikan bahwa sebagian besar pemesanan dilakukan dalam rentang waktu yang tidak terlalu jauh dari tanggal kedatangan dengan harga yang moderat.

  * **Korelasi dengan Pembatalan** : Plot menunjukkan bahwa segmen pasar "Online" memiliki proporsi pembatalan yang lebih tinggi dibandingkan segmen "Offline". Hal ini masuk akal, karena pemesanan online memberikan kemudahan untuk membatalkan. Tipe kamar tertentu juga menunjukkan tingkat pembatalan yang berbeda, menandakan bahwa fitur-fitur kategorikal ini penting untuk prediksi.

* **Pra-pemrosesan yang Robust**: Penggunaan Pipeline dan ColumnTransformer adalah praktik terbaik.

  * **RobustScaler** dipilih untuk fitur numerik, yang merupakan pilihan tepat mengingat adanya potensi outlier pada fitur seperti lead_time dan harga. Scaler ini tidak terlalu terpengaruh oleh nilai ekstrem.

  * **OneHotEncoder** digunakan untuk mengubah fitur kategorikal menjadi format numerik tanpa menciptakan urutan artifisial, yang sangat penting untuk model seperti Regresi Logistik.

# Pra-pemrosesan Data
"""

# Menghapus Booking_ID dan mendefinisikan X dan y
data = data.drop('Booking_ID', axis=1)

# Mengubah target menjadi biner
le = LabelEncoder()
data['booking_status'] = le.fit_transform(data['booking_status']) # Not_Canceled -> 1, Canceled -> 0

X = data.drop('booking_status', axis=1)
y = data['booking_status']

# Memisahkan fitur numerik dan kategorikal
numerical_features = X.select_dtypes(include=np.number).columns.tolist()
categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()

print("\nFitur Numerik:", numerical_features)
print("Fitur Kategorikal:", categorical_features)

# Membuat pipeline pra-pemrosesan
numeric_transformer = Pipeline(steps=[('scaler', RobustScaler())])
categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])

# Menggabungkan pipeline dengan ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='passthrough' # Menjaga kolom yang tidak disebutkan (jika ada)
)

# Memisahkan data training dan testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
print("\nData telah dibagi menjadi data training (70%) dan testing (30%).")

"""# Pemodelan Machine Learning & Optimasi Hyperparameter"""

# Definisikan model yang akan digunakan
models = {
    "Logistic Regression": LogisticRegression(random_state=42, max_iter=1000),
    "Random Forest": RandomForestClassifier(random_state=42),
    "XGBoost": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
}

# Membuat pipeline lengkap (pra-pemrosesan + model) dan melatihnya
results = {}
for name, model in models.items():
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('classifier', model)])

    print(f"--- Melatih {name} ---")
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    results[name] = {'pipeline': pipeline, 'accuracy': accuracy, 'f1_score': f1}
    print(f"Akurasi Test: {accuracy:.4f}")
    print(f"F1-Score Test: {f1:.4f}\n")
    print(classification_report(y_test, y_pred))
    print("-" * 50)

# Optimasi Hyperparameter untuk Random Forest (model terbaik awal)
print("\n--- Optimasi Hyperparameter untuk Random Forest dengan GridSearchCV ---")
pipeline_rf = Pipeline(steps=[('preprocessor', preprocessor),
                              ('classifier', RandomForestClassifier(random_state=42))])

param_grid_rf = {
    'classifier__n_estimators': [100, 200],
    'classifier__max_depth': [10, 20, None],
    'classifier__min_samples_split': [2, 5]
}

grid_search = GridSearchCV(pipeline_rf, param_grid_rf, cv=3, scoring='f1', n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)

print(f"Parameter terbaik: {grid_search.best_params_}")
best_rf_model = grid_search.best_estimator_

"""## Evaluasi model"""

# Evaluasi model Random Forest terbaik
y_pred_best_rf = best_rf_model.predict(X_test)
print("\nLaporan Klasifikasi Random Forest setelah Optimasi:")
print(classification_report(y_test, y_pred_best_rf))
results['Optimized Random Forest'] = {'pipeline': best_rf_model,
                                      'accuracy': accuracy_score(y_test, y_pred_best_rf),
                                      'f1_score': f1_score(y_test, y_pred_best_rf)}

# Menampilkan Hasil Akhir
results_df = pd.DataFrame([(name, res['accuracy'], res['f1_score']) for name, res in results.items()],
                          columns=['Model', 'Accuracy', 'F1 Score'])

print("\n--- Ringkasan Performa Model ---")
print(results_df.sort_values(by='F1 Score', ascending=False).reset_index(drop=True))

"""## Performa Model Machine Learning

Tiga model machine learning dievaluasi, dan hasilnya menunjukkan perbedaan performa yang jelas. Metrik utama yang digunakan adalah F1-Score, yang merupakan pilihan tepat untuk masalah klasifikasi karena menyeimbangkan precision dan recall.

* **Model Terbaik** : Random Forest menjadi model dengan performa terbaik, dengan F1-Score mencapai 0.930. Ini menunjukkan kemampuannya yang sangat baik dalam menangani data tabular dan menangkap hubungan non-linear yang kompleks antar fitur.

* **Ensemble vs. Linear**: Model berbasis ensemble tree (Random Forest dan XGBoost) secara signifikan mengungguli model linear (Regresi Logistik). Ini adalah temuan kunci yang mengindikasikan bahwa hubungan antara fitur (seperti `lead_time`, segmen pasar, dan harga) dengan status pembatalan tidak bersifat linear sederhana.

* **Optimasi Hyperparameter**: Proses GridSearchCV pada Random Forest menemukan parameter optimal (`max_depth: 20, n_estimators: 200`). Menariknya, performa model yang dioptimalkan sedikit lebih rendah dari model Random Forest dengan parameter default. Hal ini bisa berarti:

  1. Parameter default dari scikit-learn sudah sangat baik untuk dataset ini.

  2. Pencarian grid mungkin perlu diperluas untuk menemukan kombinasi yang lebih baik lagi.

  3. Perbedaan performa yang sangat kecil ini (0.9299 vs 0.9281) pada dasarnya dapat diabaikan dan menunjukkan bahwa model sudah stabil.

# Pemodelan Deep Learning

## Pra-pemrosesan data untuk Deep Learning
"""

# Pra-pemrosesan data untuk Deep Learning secara terpisah
# (Meskipun bisa digabungkan, ini membuatnya lebih eksplisit)
X_processed = preprocessor.fit_transform(X)
X_train_dl, X_test_dl, y_train_dl, y_test_dl = train_test_split(X_processed, y, test_size=0.3, random_state=42, stratify=y)

input_shape_dim = X_train_dl.shape[1]
print(f"\nJumlah fitur untuk model DL setelah encoding: {input_shape_dim}")

# Definisikan callback Early Stopping
# 'monitor' akan memantau val_loss
# 'patience' berarti training akan berhenti jika val_loss tidak membaik selama 15 epoch
early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)

# Ini akan mengurangi learning rate jika 'val_loss' tidak membaik selama 5 epoch.
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2, # Faktor pengurangan learning rate (new_lr = lr * factor)
    patience=5,
    min_lr=0.00001, # Batas bawah learning rate
    verbose=1 # Memberikan notifikasi saat learning rate diturunkan
)

# Arsitektur model
dl_model = Sequential([
    keras.layers.Dense(256, activation = "relu",input_shape = [input_shape_dim]),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(256, activation = "relu"),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(256, activation = "relu"),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(1, activation="sigmoid")
])

dl_model.compile(optimizer='Adam',loss='binary_crossentropy', metrics = ['accuracy'])

result = dl_model.fit(X_train_dl, y_train_dl,
                      validation_data=(X_test_dl, y_test_dl),
                      batch_size=256,
                      epochs=1000,
                      callbacks=[early_stopping, reduce_lr], verbose=1)

"""## Evaluasi dan Visualisasi Model Deep Learning"""

# Prediksi
y_pred_dl_proba = dl_model.predict(X_test_dl)
y_pred_dl = (y_pred_dl_proba > 0.5).astype(int)

print("\nLaporan Klasifikasi Model Deep Learning:")
print(classification_report(y_test_dl, y_pred_dl))

# Plotting
history_df = pd.DataFrame(result.history)
plt.figure(figsize=(12, 5))

# Plot Akurasi
plt.subplot(1, 2, 1)
plt.plot(history_df['accuracy'], label='Training Accuracy')
plt.plot(history_df['val_accuracy'], label='Validation Accuracy')
plt.title('Akurasi Model Deep Learning')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot Loss
plt.subplot(1, 2, 2)
plt.plot(history_df['loss'], label='Training Loss')
plt.plot(history_df['val_loss'], label='Validation Loss')
plt.title('Loss Model Deep Learning')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

"""## Performa Model Deep Learning

Sebagai perbandingan, sebuah model Jaringan Saraf Tiruan (ANN) juga dibangun.

* Arsitektur & Pelatihan: Model ini memiliki arsitektur yang cukup dalam dengan tiga lapisan tersembunyi (`Dense` 256 neuron) dan `Dropout` untuk regularisasi. Penggunaan `callbacks` seperti `EarlyStopping` dan `ReduceLROnPlateau` adalah praktik terbaik untuk mencegah overfitting dan membantu model menemukan konvergensi yang baik.

* Hasil: Model Deep Learning mencapai performa yang sangat baik, dengan F1-Score 0.91 untuk kelas "Not_Canceled" dan akurasi keseluruhan 0.88.

* Visualisasi Pelatihan: Kurva akurasi dan loss menunjukkan bahwa model belajar dengan baik. Kurva validasi mengikuti kurva pelatihan dengan cukup dekat, yang menandakan tidak adanya overfitting yang parah, berkat penggunaan `Dropout` dan `EarlyStopping`.

* Perbandingan dengan ML: Meskipun performanya kuat, model Deep Learning ini sedikit di bawah Random Forest dan XGBoost. Ini adalah temuan umum pada banyak masalah klasifikasi dengan data tabular terstruktur. Model berbasis tree seringkali lebih unggul atau setidaknya sangat kompetitif dengan effort yang lebih minimal untuk tuning.

# Kesimpulan dan Model Terbaik

Berdasarkan analisis F1-Score yang merupakan metrik paling relevan, model Random Forest (dengan parameter default) adalah model terbaik untuk proyek ini. Model ini berhasil mencapai F1-Score ~0.93, yang menunjukkan keseimbangan yang sangat baik antara kemampuan mengidentifikasi reservasi yang benar-benar tidak dibatalkan (precision) dan kemampuan untuk tidak melewatkan reservasi yang seharusnya tidak dibatalkan (recall).

Keunggulan Random Forest di sini kemungkinan besar disebabkan oleh kemampuannya untuk:

* Menangkap interaksi kompleks dan non-linear antar fitur.

* Bekerja dengan baik pada kombinasi fitur numerik dan kategorikal.

* Secara inheren tahan terhadap overfitting karena sifat ensemble-nya.

**Potensi Peningkatan**

Proyek ini sudah cukup baik, namun ada beberapa area yang bisa dieksplorasi lebih lanjut:

1. **Feature Engineering**: Menciptakan fitur baru, seperti total lama menginap (`no_of_weekend_nights` + `no_of_week_nights`) atau rasio pembatalan per tamu (`no_of_previous_cancellations` / `no_of_previous_bookings_not_canceled`).

**Model Lain**: Mencoba model gradient boosting lain seperti LightGBM atau CatBoost yang seringkali lebih cepat dan terkadang lebih akurat dari XGBoost.

`Interpretasi Model`: Menggunakan teknik seperti SHAP (SHapley Additive exPlanations) pada model Random Forest terbaik untuk memahami fitur mana yang paling berpengaruh dalam prediksi pembatalan.
"""